# -*- coding: utf-8 -*-
"""CorrelatedFeaturesWine.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14kLdAY4abhYZiv6Ax5vIWhi5JrresIFi

Many Correlated Features Set - Wine
"""

!pip install lime shap

import pandas as pd
import numpy as np
np.random.seed(0)
import matplotlib.pyplot as plt

#import data, get first 100 rows
df = pd.read_csv('winequality-red[1].csv') 
df = df.iloc[:1000]
df['quality'] = df['quality'].astype(int)
df

from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.ensemble import RandomForestRegressor

Y = df['quality']
X =  df[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',
       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',
       'pH', 'sulphates', 'alcohol']]

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2)

model = RandomForestRegressor(max_depth=6, random_state=0, n_estimators=10)
model.fit(X_train, Y_train)

importances = model.feature_importances_
indices = np.argsort(importances)

features = X_train.columns
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices])
plt.yticks(range(len(indices)), [features[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()

import lime
import lime.lime_tabular

explainer = lime.lime_tabular.LimeTabularExplainer(np.array(X_train),
                    feature_names=X.columns, 
                    class_names=['quality'], 
                    verbose=True, mode='regression')

"""First Sample"""

exp = explainer.explain_instance(X_test.iloc[0], model.predict)
exp.show_in_notebook(show_table=True, show_all=False)

exp.as_pyplot_figure()

pd.DataFrame(exp.as_list())[1].sum()

"""*   Sulphates>0.73: high sulphate values positively correlate with high wine quality.
*   Total sulfur dioxide>63.0: high total sulfur dioxide values negatively correlate with high wine quality.



*   phâ‰¤3.21: low ph values positively correlate with high wine quality.
*   Green/Red color: features that have positive correlations with the target are shown in green, otherwise red.

*   Lime intercept 5.8
*   Lime prediction 5.16708852


*   Total of the intercepts -0.637
*   Intercept and total intercept sum to the the prediction. So 5.16708852

Second Sample
"""

exp = explainer.explain_instance(X_test.iloc[1], model.predict)
exp.show_in_notebook(show_table=True, show_all=False)

exp.as_pyplot_figure()

#sum ob intercepts
pd.DataFrame(exp.as_list())[1].sum()

"""*   Intercept 5.383234284631702
 
*   Total Intercept 0.8535824850917035


*   Prediction_local [6.23681677]

**SHAP**
"""

import shap
shap_values = shap.TreeExplainer(model).shap_values(X_train)
shap.summary_plot(shap_values, X_train, plot_type="bar") #use st.pyplot(shap.summary_plot...

shap.summary_plot(shap_values, X_train)

"""

*   Single feature effects on correlated data shown by plotting the feature's value against its value for all examples.  
*   Shap values are the feature's responsibility for a chnage in the model output 


"""

shap.dependence_plot("alcohol", shap_values, X_train)

shap.dependence_plot("volatile acidity", shap_values, X_train)

shap.dependence_plot("total sulfur dioxide", shap_values, X_train, show=False)

#with feature_perterbation set to tree_path_dependent
shap_interaction_values = shap.TreeExplainer(model).shap_interaction_values(X_train.iloc[:,:])

shap.summary_plot(shap_interaction_values, X_train.iloc[:,:])

"""

*   Alcohol: has positive impact on the quality rating.
*   pH: has a negative impact on the quality rating


*   Sulphates: is positively related to the quality rating 




"""

